
replicaCount: 3

podDisruptionBudget:
  maxUnavailable: 1

image:
  repository: confluentinc/cp-kafka
  tag: 4.0.1
  pullPolicy: IfNotPresent

## Client service.
service:
  type: ClusterIP
  annotations: {}
    ## AWS example for use with LoadBalancer service type.
    # external-dns.alpha.kubernetes.io/hostname: kafka.cluster.local
    # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    # service.beta.kubernetes.io/aws-load-balancer-internal: "true"
  ports:
    client:
      port: 9092
      targetPort: client
      protocol: TCP

ports:
  client:
    containerPort: 9092
    protocol: TCP

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 1024Mi
  # requests:
  #   cpu: 100m
  #   memory: 1024Mi

## OnDelete requires you to manually delete each pod when making updates.
## This approach is at the moment safer than RollingUpdate because replication
## may be incomplete when replication source pod is killed.
##
## ref: http://blog.kubernetes.io/2017/09/kubernetes-statefulsets-daemonsets.html
updateStrategy:
  type: OnDelete

## Start and stop pods in Parallel or OrderedReady (one-by-one.)  Note - Can not change after first release.
## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
podManagementPolicy: OrderedReady

## If RBAC is enabled on the cluster, the Kafka init container needs a service account
## with permissisions sufficient to apply pod labels
rbac:
  enabled: false

## Pod scheduling preferences (by default keep pods within a release on separate nodes).
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## By default we don't set affinity
affinity: {}
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 50
  #       podAffinityTerm:
  #         topologyKey: failure-domain.beta.kubernetes.io/zone
  #         labelSelector:
  #           matchLabels:
  #             release: kafka
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     - weight: 40
  #       topologyKey: "kubernetes.io/hostname"
  #       labelSelector:
  #         matchLabels:
  #           release: kafka

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/path: "/metrics"
  # prometheus.io/port: "9404"  # or "9308"

podLabels: {}
  # team: "developers"
  # service: "kafka"

## Liveness probe config.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
##
livenessProbe:
  exec:
    command:
      - sh
      - -ec
      - /usr/bin/jps | /bin/grep -q SupportedKafka
  initialDelaySeconds: 30
  # periodSeconds: 10
  # timeoutSeconds: 5
  # failureThreshold: 3
  # successThreshold: 1

## Readiness probe config.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
##
readinessProbe:
  tcpSocket:
    port: client
  initialDelaySeconds: 30
  # periodSeconds: 10
  # timeoutSeconds: 5
  # failureThreshold: 3
  # successThreshold: 1

## Period to wait for broker graceful shutdown (sigterm) before pod is killed (sigkill)
## ref: https://kubernetes-v1-4.github.io/docs/user-guide/production-pods/#lifecycle-hooks-and-termination-notice
## ref: https://kafka.apache.org/10/documentation.html#brokerconfigs controlled.shutdown.*
terminationGracePeriodSeconds: 60

## Tolerations for nodes that have taints on them.
## Useful if you want to dedicate nodes to just run kafka
## https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []
# tolerations:
# - key: "key"
#   operator: "Equal"
#   value: "value"
#   effect: "NoSchedule"

securityContext: {}

persistence:
  enabled: true
  ## kafka data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  accessMode: ReadWriteOnce
  ## The size of the PersistentVolume to allocate to each Kafka Pod in the StatefulSet. For
  ## production servers this number should likely be much larger.
  ##
  size: 1Gi
  ## The location within the Kafka container where the PV will mount its storage and Kafka will
  ## store its logs.
  ##
  mountPath: /opt/kafka/data

## Use an alternate scheduler, e.g. "stork".
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
# schedulerName: stork

## The subpath within the Kafka container's PV where logs will be stored.
## This is combined with `persistence.mountPath`, to create, by default: /opt/kafka/data/logs
logSubPath: "logs"

## External access.
##
external:
  enabled: false
  firstListenerPort: 31090
  domain: cluster.local
  service:
    dnsShortname: kafka
    port: 19092

## Init container. Adds pod labels.
##
init:
  image: "lwolf/kubectl_deployer"
  imageTag: "0.4"
  imagePullPolicy: "IfNotPresent"

## rackAwareness requires at least Kubernetes 1.9.
## refs:
## - https://github.com/Yolean/kubernetes-kafka/pull/41
## - https://github.com/kubernetes/kubernetes/issues/19367#issuecomment-371278421
## - https://github.com/kubernetes/kubernetes/issues/31984#issuecomment-253682845
## - https://github.com/kubernetes/kubernetes/issues/62078
## - https://issues.apache.org/jira/browse/KAFKA-1215
rackAwareness:
  enabled: false

## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
## here in map format, as defined in the official docs.
##
## ref: https://kafka.apache.org/documentation/#brokerconfigs
configurationOverrides:
  "offsets.topic.replication.factor": 3
  # "auto.leader.rebalance.enable": true
  # "auto.create.topics.enable": true
  # "controlled.shutdown.enable": true
  # "controlled.shutdown.max.retries": 100

  ## Options required for external access via NodePort
  ## ref:
  ## - http://kafka.apache.org/documentation/#security_configbroker
  ## - https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
  ##
  ## Setting "advertised.listeners" here appends to "PLAINTEXT://${POD_IP}:9092,"
  # "advertised.listeners": |-
  #   EXTERNAL://kafka.cluster.local:$((31090 + ${KAFKA_BROKER_ID}))
  # "listener.security.protocol.map": |-
  #   PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT

env:
  KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
  KAFKA_JMX_PORT: 1099

exporters:

  jmx:
    enabled: false
    image:
      repository: sscaling/jmx-prometheus-exporter
      tag: 0.3.0
      pullPolicy: IfNotPresent
    config:
      ## ref: https://github.com/prometheus/jmx_exporter/blob/master/example_configs/kafka-0-8-2.yml
      lowercaseOutputName: false
      startDelaySeconds: 30
      rules:
        - pattern : kafka.cluster<type=(.+), name=(.+), topic=(.+), partition=(.+)><>Value
          name: kafka_cluster_$1_$2
          labels:
            topic: "$3"
            partition: "$4"
        - pattern : kafka.log<type=Log, name=(.+), topic=(.+), partition=(.+)><>Value
          name: kafka_log_$1
          labels:
            topic: "$2"
            partition: "$3"
        - pattern : kafka.controller<type=(.+), name=(.+)><>(Count|Value)
          name: kafka_controller_$1_$2
        - pattern : kafka.network<type=(.+), name=(.+)><>Value
          name: kafka_network_$1_$2
        - pattern : kafka.network<type=(.+), name=(.+)PerSec, request=(.+)><>Count
          name: kafka_network_$1_$2_total
          labels:
            request: "$3"
        - pattern : kafka.network<type=(.+), name=(\w+), networkProcessor=(.+)><>Count
          name: kafka_network_$1_$2
          labels:
            request: "$3"
          type: COUNTER
        - pattern : kafka.network<type=(.+), name=(\w+), request=(\w+)><>Count
          name: kafka_network_$1_$2
          labels:
            request: "$3"
        - pattern : kafka.network<type=(.+), name=(\w+)><>Count
          name: kafka_network_$1_$2
        - pattern : kafka.server<type=(.+), name=(.+)PerSec\w*, topic=(.+)><>Count
          name: kafka_server_$1_$2_total
          labels:
            topic: "$3"
        - pattern : kafka.server<type=(.+), name=(.+)PerSec\w*><>Count
          name: kafka_server_$1_$2_total
          type: COUNTER
        - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>(Count|Value)
          name: kafka_server_$1_$2
          labels:
            clientId: "$3"
            topic: "$4"
            partition: "$5"
        - pattern : kafka.server<type=(.+), name=(.+), topic=(.+), partition=(.*)><>(Count|Value)
          name: kafka_server_$1_$2
          labels:
            topic: "$3"
            partition: "$4"
        - pattern : kafka.server<type=(.+), name=(.+), topic=(.+)><>(Count|Value)
          name: kafka_server_$1_$2
          labels:
            topic: "$3"
          type: COUNTER
        - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>(Count|Value)
          name: kafka_server_$1_$2
          labels:
            clientId: "$3"
            broker: "$4:$5"
        - pattern : kafka.server<type=(.+), name=(.+), clientId=(.+)><>(Count|Value)
          name: kafka_server_$1_$2
          labels:
            clientId: "$3"
        - pattern : kafka.server<type=(.+), name=(.+)><>(Count|Value)
          name: kafka_server_$1_$2
        - pattern : kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
          name: kafka_$1_$2_$3_total
        - pattern : kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, topic=(.+)><>Count
          name: kafka_$1_$2_$3_total
          labels:
            topic: "$4"
          type: COUNTER
        - pattern : kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, topic=(.+), partition=(.+)><>Count
          name: kafka_$1_$2_$3_total
          labels:
            topic: "$4"
            partition: "$5"
          type: COUNTER
        - pattern : kafka.(\w+)<type=(.+), name=(.+)><>(Count|Value)
          name: kafka_$1_$2_$3_$4
          type: COUNTER
        - pattern : kafka.(\w+)<type=(.+), name=(.+), (\w+)=(.+)><>(Count|Value)
          name: kafka_$1_$2_$3_$6
          labels:
            "$4": "$5"
    env: {}
    resources: {}
    path: /metrics
    targetPort:
    ports:
      jmxxp:
        containerPort: 9404
        protocol: TCP
    livenessProbe:
      tcpSocket:
        port: jmxxp
      initialDelaySeconds: 30
      periodSeconds: 15
      timeoutSeconds: 60
      failureThreshold: 8
      successThreshold: 1
    readinessProbe:
      tcpSocket:
        port: jmxxp
      initialDelaySeconds: 30
      periodSeconds: 15
      timeoutSeconds: 60
      failureThreshold: 8
      successThreshold: 1

  kafka:
  ## refs:
  ## - https://github.com/danielqsj/kafka_exporter
  ## - https://hub.docker.com/r/danielqsj/kafka-exporter/
  ## - https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/
    enabled: false
    image:
      repository: danielqsj/kafka-exporter
      tag: v1.0.1
      pullPolicy: IfNotPresent
    config:
      topicFilter: ".*"
      logLevel: info
    env: {}
    resources: {}
    path: /metrics
    initialContainerDelaySeconds: 60
    ports:
      kafkaxp:
        containerPort: 9308
        protocol: TCP
    livenessProbe:
      tcpSocket:
        port: kafkaxp
      initialDelaySeconds: 60
      periodSeconds: 15
      timeoutSeconds: 60
      failureThreshold: 8
      successThreshold: 1
    readinessProbe:
      tcpSocket:
        port: kafkaxp
      initialDelaySeconds: 60
      periodSeconds: 15
      timeoutSeconds: 60
      failureThreshold: 8
      successThreshold: 1

zookeeper:
  ## If true, install the Zookeeper chart alongside Kafka
  ## ref: https://github.com/kubernetes/charts/tree/master/incubator/zookeeper
  enabled: true

  ## Configure Zookeeper resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: ~

  persistence:
    ## The size of the PersistentVolume to allocate to each Zookeeper Pod in the StatefulSet. For
    ## production servers this number should likely be much larger.
    size: 2Gi

  env:
    ## The JVM heap size.
    ZK_HEAP_SIZE: 2G

  ## Specify Zookeeper image pull policy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  image:
    repository: confluentinc/cp-kafka
    tag: 4.0.0
    pullPolicy: IfNotPresent

  ports:
    client:
      containerPort: 2181

  ## Kafka's "zookeeper.connect" string. Most useful when Zookeeper subchart is disabled and a
  ## discrete zookeeper is to be used.
  # connect: zookeeper:2181/kafka
