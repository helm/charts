# ------------------------------------------------------------------------------
# Fabric Orderer
# ------------------------------------------------------------------------------
## Default values for hlf-ord.
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.
image:
  repository: hyperledger/fabric-orderer
  tag: 1.2.0
  pullPolicy: IfNotPresent

service:
  # Cluster IP or LoadBalancer
  type: ClusterIP
  port: 7050

persistence:
  enabled: true
  annotations: {}
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 1Gi
  # existingClaim: ""

##################################
## Further configuration options #
##################################
## Address of Certificate Authority where O
caAddress: hlf-ca.local
## Username for registering/enrolling with CA
caUsername: ord1
## Password for registering/enrolling with CA  (defaults to random 24 alphanumeric)
# caPassword:

ord:
  # Tools version
  hlfToolsVersion: 1.2.0
  ## Type of Orderer, `solo` or `kafka`
  type: solo
  ## MSP ID of the Orderer
  mspID: OrdererMSP

secrets: {}
  ## This should contain "genesis" block derived from a configtx.yaml
  ## configtxgen -profile OrdererGenesis -outputBlock genesis.block
  # genesis: hlf--genesis
  ## This should contain the Certificate of the Orderer Organisation admin
  ## This is necessary to successfully run the orderer
  # adminCert: hlf--ord-admincert
  ## This should contain the CA server's TLS details under the key tls.crt (e.g. a Let's Encrypt Certificate PEM)
  # caServerTls: ca--tls

resources: {}
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}
  ## Suggested antiAffinity, as each Orderer should be on a separate Node for resilience
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     - topologyKey: "kubernetes.io/hostname"
  #       labelSelector:
  #         matchLabels:
  #           app: hlf-ord

# ------------------------------------------------------------------------------
# Kafka:
# ref: https://github.com/helm/charts/blob/master/incubator/kafka/values.yaml
# ------------------------------------------------------------------------------
kafka:
  enabled: false
  # Minimum number of nodes necessary in order to exhibit crash fault tolerance
  # With 4 brokers, you can have 1 broker go down, all channels will continue to be writeable and readable, and new channels can be created.)
  # ref: https://hyperledger-fabric.readthedocs.io/en/release-1.1/kafka.html?highlight=orderer#bringing-up-a-kafka-based-ordering-service
  replicas: 4

  ## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
  ## here in map format, as defined in the official docs.
  ## ref: https://kafka.apache.org/documentation/#brokerconfigs
  ##
  configurationOverrides:
    "offsets.topic.replication.factor": 3
    # "auto.leader.rebalance.enable": true
    # "controlled.shutdown.enable": true
    # "controlled.shutdown.max.retries": 100
    "auto.create.topics.enable": true  # Useful to enable the Node.js client to create topics as required

    # NOTE: The below are required for Hyperledger Fabric orderer to work (but last one is problematic for normal setups - best to keep separate Kafka clusters for logs/HLF)
    "default.replication.factor": 3
    "unclean.leader.election.enable": false
    "min.insync.replicas": 2
    "message.max.bytes": "103809024"  # 99 * 1024 * 1024 B
    "replica.fetch.max.bytes": "103809024"  # 99 * 1024 * 1024 B
    "log.retention.ms": -1  # This should be only used for the HL Fabric Orderer (which needs to keep all logs)

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we don't set affinity
  affinity: {}
  ## Alternatively, this typical example defines:
  ## antiAffinity (to keep Kafka pods on separate pods)
  ## and affinity (to encourage Kafka pods to be collocated with Zookeeper pods)
  # affinity:
  #   podAntiAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #     - labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: In
  #           values:
  #           - kafka
  #       topologyKey: "kubernetes.io/hostname"
  #   podAffinity:
  #     preferredDuringSchedulingIgnoredDuringExecution:
  #      - weight: 50
  #        podAffinityTerm:
  #          labelSelector:
  #            matchExpressions:
  #            - key: app
  #              operator: In
  #              values:
  #                - zookeeper
  #          topologyKey: "kubernetes.io/hostname"
