# Default values for sentry.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  repository: sentry
  tag: "9.1.1"
  pullPolicy: IfNotPresent
  # Add the secret name to pull from a private registry.
  imagePullSecrets: []
  # - name:

# How many web UI instances to run
web:
  replicacount: 1
  resources:
    limits:
      cpu: 500m
      memory: 500Mi
    requests:
      cpu: 300m
      memory: 300Mi
  env:
    - name: GITHUB_APP_ID
      value:
    - name: GITHUB_API_SECRET
      value:
  nodeSelector: {}
  tolerations: []
  affinity: {}
  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []


# How many cron instances to run
cron:
  replicacount: 1
  resources:
    limits:
      cpu: 200m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []

# How many worker instances to run
worker:
  replicacount: 2
  resources:
    limits:
      cpu: 300m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  # concurrency:

# Admin user to create
user:
  # Indicated to create the admin user or not,
  # Default is true as the initial installation.
  create: true
  email: admin@sentry.local

# BYO Email server
# TODO: Add exim4 template
# https://docs.sentry.io/server/installation/docker/#outbound-email
email:
  from_address: sentry@sentry.local
  host: smtp
  port: 25
  use_tls: false
  user:
  password:
  enable_replies: false

# Name of the service and what port to expose on the pod
# Don't change these unless you know what you're doing
service:
  name: sentry
  type: LoadBalancer
  externalPort: 9000
  internalPort: 9000

  ## Service annotations
  ##
  annotations: {}

  ## External IP addresses of service
  ## Default: nil
  ##
  # externalIPs:
  # - 192.168.0.1

  ## Load Balancer allow-list
  # loadBalancerSourceRanges: []

# Configure the location of Sentry artifacts
filestore:
  # Set to one of filesystem, gcs or s3 as supported by Sentry.
  backend: filesystem

  filesystem:
    path: /var/lib/sentry/files

    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      enabled: true
      ## database data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      # storageClass: "-"
      accessMode: ReadWriteOnce
      size: 10Gi

      ## Whether to mount the persistent volume to the Sentry worker and
      ## cron deployments. This setting needs to be enabled for some advanced
      ## Sentry features, such as private source maps. If you disable this
      ## setting, the Sentry workers will not have access to artifacts you upload
      ## through the web deployment.
      ## Please note that you may need to change your accessMode to ReadWriteMany
      ## if you plan on having the web, worker and cron deployments run on
      ## different nodes.
      persistentWorkers: false

  ## Point this at a pre-configured secret containing a service account. The resulting
  ## secret will be mounted at /var/run/secrets/google
  gcs:
    credentialsFile: credentials.json
  #  secretName:
  #  bucketName:

  ## Currently unconfigured and changing this has no impact on the template configuration.
  s3: {}
  #  accessKey:
  #  secretKey:
  #  bucketName:

## Configure ingress resource that allow you to access the
## Sentry installation. Set up the URL
## ref: http://kubernetes.io/docs/user-guide/ingress/
##
ingress:
  enabled: false
  hostname: sentry.local

  ## Ingress annotations
  ##
  annotations: {}
  #   kubernetes.io/ingress.class: nginx
  #   kubernetes.io/tls-acme: 'true'

  ## Ingress TLS configuration
  ## Secrets must be manually created in the namespace
  ##
  tls: []
  #   - secretName: sentry.local-tls
  #     hosts:
  #       - sentry.local

# TODO: add support for plugins https://docs.sentry.io/server/plugins/

postgresql:
  enabled: true
  postgresDatabase: sentry
  postgresUser: sentry
  # Only used when internal PG is disabled
  # postgresHost: postgres
  # postgresPassword: postgres
  # postgresPort: 5432
  imageTag: "9.6"
  persistence:
    enabled: true

redis:
  enabled: true
  # Only used when internal redis is disabled
  # host: redis
  # Just omit the password field if your redis cluster doesn't use password
  # password: redis
  # port: 6379
  master:
    persistence:
      enabled: true
      # If change pvc size redis.master.persistence.size: 20Gi

config:
  configYml: ""
  sentryConfPy: ""

## Prometheus Exporter / Metrics
##
metrics:
  enabled: false

  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

  ## Metrics exporter resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 100m
  #     memory: 100Mi

  nodeSelector: {}
  tolerations: []
  affinity: {}
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  service:
    type: ClusterIP
    labels: {}

  image:
    repository: prom/statsd-exporter
    tag: v0.10.5
    pullPolicy: IfNotPresent

  # Enable this if you're using https://github.com/coreos/prometheus-operator
  serviceMonitor:
    enabled: false
    ## Specify a namespace if needed
    # namespace: monitoring
    # fallback to the prometheus default unless specified
    # interval: 10s
    ## Defaults to what's used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)
    ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)
    ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)
    selector:
      prometheus: kube-prometheus

# Provide affinity for hooks if needed
hooks:
  affinity: {}
  dbInit:
    resources:
    # We setup 3000Mi for the memory limit because of a Sentry instance need at least 3Gb RAM to perform a migration process
    # reference: https://github.com/helm/charts/issues/15296
      limits:
        memory: 3200Mi
      requests:
        memory: 3100Mi
