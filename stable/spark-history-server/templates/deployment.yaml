apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "spark-history-server.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "spark-history-server.name" . }}
    helm.sh/chart: {{ include "spark-history-server.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "spark-history-server.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "spark-history-server.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      serviceAccountName: {{ include "spark-history-server.serviceAccountName" . }}
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        env:
        - name: HADOOP_CONF_DIR
          value: {{ .Values.hdfs.HADOOP_CONF_DIR }}
        - name: SPARK_NO_DAEMONIZE
          value: "true"
        ports:
        - name: historyport
          containerPort: {{ .Values.service.port }}
          protocol: TCP
        command:
        - "/bin/sh"
        - "-c"
        - >
          if [ "$enablePVC" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=file:/mnt/$eventsDir";
          elif [ "$enableGCS" == "true" ]; then
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.hadoop.google.cloud.auth.service.account.json.keyfile=/etc/secrets/$key \
            -Dspark.history.fs.logDirectory=$logDirectory";
          else:
            export SPARK_HISTORY_OPTS="$SPARK_HISTORY_OPTS \
            -Dspark.history.fs.logDirectory=$logDirectory";
          fi;
          /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer;
        envFrom:
        - configMapRef:
            name: {{ include "spark-history-server.fullname" . }}
        livenessProbe:
          httpGet:
            path: /
            port: historyport
        readinessProbe:
          httpGet:
            path: /
            port: historyport
        volumeMounts:
        {{- if .Values.pvc.enablePVC }}
        - name: data
          mountPath: /mnt/{{ .Values.pvc.eventsDir }}
        {{- else if .Values.gcs.enableGCS }}
        - name: secrets-volume
          mountPath: /etc/secrets
        {{- else }}
        - name: core-site
          mountPath: /etc/hadoop/core-site.xml
          subPath: core-site.xml
        - name: hdfs-site
          mountPath: /etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
        {{- end }}
      volumes:
      {{- if .Values.pvc.enablePVC }}
      - name: data
        persistentVolumeClaim:
          claimName: {{ .Values.pvc.existingClaimName }}
      {{- else if .Values.gcs.enableGCS }}
      - name: secrets-volume
        secret:
          secretName: {{ .Values.gcs.secret }}
      {{- else }}
      - name: hdfs-site
        configMap:
          name: {{ .Values.hdfs.hdfsSiteConfigMap }}
      - name: core-site
        configMap:
          name: {{ .Values.hdfs.coreSiteConfigMap }}
      {{- end }}
