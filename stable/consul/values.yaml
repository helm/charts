# Default values for consul.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

## Consul service ports
httpPort: 8500
rpcPort: 8400
serflanPort: 8301
serflanUdpPort: 8301
serfwanPort: 8302
serfwanUdpPort: 8302
serverPort: 8300
consulDnsPort: 8600

## Specify the domain with which consul should run with
## This will be passed as a -domain parameter
domain: consul

## Used as selector
component: "consul"
replicas: 3
image:
  repository: "consul"
  tag: "1.0.0"
  pullPolicy: "Always"
resource: {}
 # requests:
 #   cpu: "100m"
 #   memory: "256Mi"
 # limits:
 #   cpu: "500m"
 #   memory: "512Mi"
## Persistent volume size
storage: "1Gi"

## Needed for 0.8.0 and later IF all consul containers are spun up
## on the same machine. Without this they all generate the same
## host id.
disableHostNodeId: false

## Datacenter name for consul. If not supplied, will use the consul
## default 'dc1'
# datacenterName: dc1

## Encrypt Gossip traffic
encryptGossip: true

## predefined value for gossip key.
## Will use a generated random alpha numeric if not provided
# gossipKey: key

## consul data Persistent Volume storage Class
## If defined, storageClassName: <storageClass>
## If set to "-", storageClassName: "", which disables dynamic provisioning
## If undefined (the default) or set to null, no storageClassName spec is
##   set, choosing the default provisioner.  (gp2 on AWS, standard on
##   GKE, AWS & OpenStack)
##
# storageClass: "-"

## Setting maxUnavailable will create a pod disruption budget that will prevent
## voluntarty cluster administration from taking down too many consul pods. If
## you set maxUnavailable, you should set it to ceil((n/2) - 1), where
## n = replicas. For example, if you have 5 or 6 replicas, you'll want to set
## maxUnavailable = 2. If you are using the default of 3 replicas, you'll want
## to set maxUnavailable to 1.
maxUnavailable: 1

## nodeAffinity settings
# nodeAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#     - matchExpressions:
#       - key: cloud.google.com/gke-preemptible
#         operator: NotIn
#         values:
#         - true

## Anti-Affinity setting. The default "hard" will use pod anti-affinity that is
## requiredDuringSchedulingIgnoredDuringExecution to ensure 2 services don't
## end up on the same node. Setting this to "soft" will use
## preferredDuringSchedulingIgnoredDuringExecution. If set to anything else,
## no anti-affinity rules will be configured.
antiAffinity: "hard"

## Enable Consul Web UI
##
ui:
  enabled: true
## Create dedicated UI service
##
uiService:
  enabled: true
  type: "NodePort"

## Create an Ingress for the Web UI
uiIngress:
  enabled: false
  annotations: {}
  hosts: []
  tls: {}

## test container details
test:
  image:
    repository: lachlanevenson/k8s-kubectl
    tag: v1.4.8-bash
