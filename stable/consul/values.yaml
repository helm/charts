# Default values for consul.
# This is a YAML-formatted file.
# Declare name/value pairs to be passed into your templates.
# name: value

## Consul service ports
HttpPort: 8500
RpcPort: 8400
SerflanPort: 8301
SerflanUdpPort: 8301
SerfwanPort: 8302
SerfwanUdpPort: 8302
ServerPort: 8300
ConsulDnsPort: 8600

## Specify the domain with which consul should run with
## This will be passed as a -domain parameter
Domain: consul

## Used as selector
Component: "consul"
Replicas: 3
Image: "consul"
ImageTag: "1.0.0"
ImagePullPolicy: "Always"
Resources: {}
 # requests:
 #   cpu: "100m"
 #   memory: "256Mi"
 # limits:
 #   cpu: "500m"
 #   memory: "512Mi"
## Persistent volume size
Storage: "1Gi"

## Needed for 0.8.0 and later IF all consul containers are spun up
## on the same machine. Without this they all generate the same
## host id.
DisableHostNodeId: false

## Datacenter name for consul. If not supplied, will use the consul
## default 'dc1'
# DatacenterName: dc1

## Encrypt Gossip traffic
EncryptGossip: true

## predefined value for gossip key.
## Will use a generated random alpha numeric if not provided
# GossipKey: key

## consul data Persistent Volume Storage Class
## If defined, StorageClassName: <storageClass>
## If set to "-", StorageClassName: "", which disables dynamic provisioning
## If undefined (the default) or set to null, no storageClassName spec is
##   set, choosing the default provisioner.  (gp2 on AWS, standard on
##   GKE, AWS & OpenStack)
##
# StorageClass: "-"

## Setting maxUnavailable will create a pod disruption budget that will prevent
## voluntarty cluster administration from taking down too many consul pods. If
## you set maxUnavailable, you should set it to ceil((n/2) - 1), where
## n = Replicas. For example, if you have 5 or 6 Replicas, you'll want to set
## maxUnavailable = 2. If you are using the default of 3 Replicas, you'll want
## to set maxUnavailable to 1.
maxUnavailable: 1

## nodeAffinity settings
# nodeAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#     - matchExpressions:
#       - key: cloud.google.com/gke-preemptible
#         operator: NotIn
#         values:
#         - true

## Anti-Affinity setting. The default "hard" will use pod anti-affinity that is
## requiredDuringSchedulingIgnoredDuringExecution to ensure 2 services don't
## end up on the same node. Setting this to "soft" will use
## preferredDuringSchedulingIgnoredDuringExecution. If set to anything else,
## no anti-affinity rules will be configured.
antiAffinity: "hard"

## Enable Consul Web UI
##
ui:
  enabled: true
## Create dedicated UI service
##
uiService:
  enabled: true
  type: "NodePort"

## Create an Ingress for the Web UI
uiIngress:
  enabled: false
  annotations: {}
  hosts: []
  tls: {}

## test container details
test:
  image: lachlanevenson/k8s-kubectl
  imageTag: v1.4.8-bash
